{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Basics of Neural Networks\n",
    "* <b>Learning Objective:</b> In this problem, you are asked to implement a basic multi-layer fully connected neural network from scratch, including forward and backward passes of certain essential layers, to perform an image classification task on the CIFAR100 dataset. You need to implement essential functions in different indicated python files under directory `lib`.\n",
    "* <b>Provided Code:</b> We provide the skeletons of classes you need to complete. Forward checking and gradient checkings are provided for verifying your implementation as well.\n",
    "* <b>TODOs:</b> You are asked to implement the forward passes and backward passes for standard layers and loss functions, various widely-used optimizers, and part of the training procedure. And finally we want you to train a network from scratch on your own. Also, there are inline questions you need to answer. See `README.md` to set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.mlp.fully_conn import *\n",
    "from lib.mlp.layer_utils import *\n",
    "from lib.datasets import *\n",
    "from lib.mlp.train import *\n",
    "from lib.grad_check import *\n",
    "from lib.optim import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (CIFAR-100 with 20 superclasses)\n",
    "\n",
    "In this homework, we will be classifying images from the CIFAR-100 dataset into the 20 superclasses. More information about the CIFAR-100 dataset and the 20 superclasses can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Download the CIFAR-100 data files [here](https://drive.google.com/drive/folders/1imXxTnpkMbWEe41pkAGNt_JMTXECDSaW?usp=share_link), and save the `.mat` files to the `data/cifar100` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CIFAR100_data('data/cifar100/')\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print (\"Name: {} Shape: {}, {}\".format(k, v.shape, type(v)))\n",
    "    else:\n",
    "        print(\"{}: {}\".format(k, v))\n",
    "label_names = data['label_names']\n",
    "mean_image = data['mean_image'][0]\n",
    "std_image = data['std_image'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Standard Layers\n",
    "You will now implement all the following standard layers commonly seen in a fully connected neural network (aka multi-layer perceptron, MLP). Please refer to the file `lib/mlp/layer_utils.py`. Take a look at each class skeleton, and we will walk you through the network layer by layer. We provide results of some examples we pre-computed for you for checking the forward pass, and also the gradient checking for the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Forward [2pt]\n",
    "In the class skeleton `flatten` and `fc` in `lib/mlp/layer_utils.py`, please complete the forward pass in function `forward`. The input to the `fc` layer may not be of dimension (batch size, features size), it could be an image or any higher dimensional data. We want to convert the input to have a shape of (batch size, features size). Make sure that you handle this dimensionality issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the fc forward function\n",
    "input_bz = 3 # batch size\n",
    "input_dim = (7, 6, 4)\n",
    "output_dim = 4\n",
    "\n",
    "input_size = input_bz * np.prod(input_dim)\n",
    "weight_size = output_dim * np.prod(input_dim)\n",
    "\n",
    "flatten_layer = flatten(name=\"flatten_test\")\n",
    "single_fc = fc(np.prod(input_dim), output_dim, init_scale=0.02, name=\"fc_test\")\n",
    "\n",
    "x = np.linspace(-0.1, 0.4, num=input_size).reshape(input_bz, *input_dim)\n",
    "w = np.linspace(-0.2, 0.2, num=weight_size).reshape(np.prod(input_dim), output_dim)\n",
    "b = np.linspace(-0.3, 0.3, num=output_dim)\n",
    "\n",
    "single_fc.params[single_fc.w_name] = w\n",
    "single_fc.params[single_fc.b_name] = b\n",
    "\n",
    "out = single_fc.forward(flatten_layer.forward(x))\n",
    "\n",
    "correct_out = np.array([[0.63910291, 0.83740057, 1.03569824, 1.23399591],\n",
    "                        [0.61401587, 0.82903823, 1.04406058, 1.25908294],\n",
    "                        [0.58892884, 0.82067589, 1.05242293, 1.28416997]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-8\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Backward [2pt]\n",
    "Please complete the function `backward` as the backward pass of the `flatten` and `fc` layers. Follow the instructions in the comments to store gradients into the predefined dictionaries in the attributes of the class. Parameters of the layer are also stored in the predefined dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the fc backward function\n",
    "inp = np.random.randn(15, 2, 2, 3)\n",
    "w = np.random.randn(12, 15)\n",
    "b = np.random.randn(15)\n",
    "dout = np.random.randn(15, 15)\n",
    "\n",
    "flatten_layer = flatten(name=\"flatten_test\")\n",
    "x = flatten_layer.forward(inp)\n",
    "single_fc = fc(np.prod(x.shape[1:]), 15, init_scale=5e-2, name=\"fc_test\")\n",
    "single_fc.params[single_fc.w_name] = w\n",
    "single_fc.params[single_fc.b_name] = b\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: single_fc.forward(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: single_fc.forward(x), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: single_fc.forward(x), b, dout)\n",
    "\n",
    "out = single_fc.forward(x)\n",
    "dx = single_fc.backward(dout)\n",
    "dw = single_fc.grads[single_fc.w_name]\n",
    "db = single_fc.grads[single_fc.b_name]\n",
    "dinp = flatten_layer.backward(dx)\n",
    "\n",
    "# The error should be around 1e-9\n",
    "print(\"dx Error: \", rel_error(dx_num, dx))\n",
    "# The errors should be around 1e-10\n",
    "print(\"dw Error: \", rel_error(dw_num, dw))\n",
    "print(\"db Error: \", rel_error(db_num, db))\n",
    "# The shapes should be same\n",
    "print(\"dinp Shape: \", dinp.shape, inp.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeLU Forward [2pt]\n",
    "In the class skeleton `gelu` in `lib/mlp/layer_utils.py`, please complete the `forward` pass.\n",
    "\n",
    "GeLU is a smooth version of ReLU and it's used in pre-training LLMs such as GPT-3 and BERT. \n",
    "\n",
    "$$\n",
    "\\mathrm{GeLU}(x) = x \\Phi(x) \\approx 0.5x(1+\\tanh(\\sqrt{2/\\pi}(x+0.044715x^3)))\n",
    "$$\n",
    "\n",
    "Where $\\Phi(x)$ is the CDF for standard Gaussian random variables.  You should use the approximate version to compute forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the leaky_relu forward function\n",
    "x = np.linspace(-1.5, 1.5, num=12).reshape(3, 4)\n",
    "gelu_f = gelu(name=\"gelu_f\")\n",
    "\n",
    "out = gelu_f.forward(x)\n",
    "correct_out = np.array([[-0.10042842, -0.13504766, -0.16231757, -0.1689214 ],\n",
    "                        [-0.13960493, -0.06078651,  0.07557713,  0.26948598],\n",
    "                        [ 0.51289678,  0.79222788,  1.09222506,  1.39957158]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-7\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeLU Backward [2pt]\n",
    "Please complete the `backward` pass of the class `gelu`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the relu backward function\n",
    "x = np.random.randn(15, 15)\n",
    "dout = np.random.randn(*x.shape)\n",
    "gelu_b = gelu(name=\"gelu_b\")\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: gelu_b.forward(x), x, dout)\n",
    "\n",
    "out = gelu_b.forward(x)\n",
    "dx = gelu_b.backward(dout)\n",
    "\n",
    "# The error should not be larger than 1e-4, since we are using an approximate version of GeLU activation. \n",
    "print (\"dx Error: \", rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Forward [2pt]\n",
    "In the class `dropout` in `lib/mlp/layer_utils.py`, please complete the `forward` pass.  \n",
    "Remember that the dropout is **only applied during training phase**, you should pay attention to this while implementing the function.\n",
    "##### Important Note1: The probability argument input to the function is the \"keep probability\": probability that each activation is kept.\n",
    "##### Important Note2: If the keep_prob is set to 1, make it as no dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "x = np.random.randn(100, 100) + 5.0\n",
    "\n",
    "print (\"----------------------------------------------------------------\")\n",
    "for p in [0, 0.25, 0.50, 0.75, 1]:\n",
    "    dropout_f = dropout(keep_prob=p)\n",
    "    out = dropout_f.forward(x, True)\n",
    "    out_test = dropout_f.forward(x, False)\n",
    "\n",
    "    # Mean of output should be similar to mean of input\n",
    "    # Means of output during training time and testing time should be similar\n",
    "    print (\"Dropout Keep Prob = \", p)\n",
    "    print (\"Mean of input: \", x.mean())\n",
    "    print (\"Mean of output during training time: \", out.mean())\n",
    "    print (\"Mean of output during testing time: \", out_test.mean())\n",
    "    print (\"Fraction of output set to zero during training time: \", (out == 0).mean())\n",
    "    print (\"Fraction of output set to zero during testing time: \", (out_test == 0).mean())\n",
    "    print (\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Backward [2pt]\n",
    "Please complete the `backward` pass. Again remember that the dropout is only applied during training phase, handle this in the backward pass as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "x = np.random.randn(5, 5) + 5\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "keep_prob = 0.75\n",
    "dropout_b = dropout(keep_prob, seed=100)\n",
    "out = dropout_b.forward(x, True, seed=1)\n",
    "dx = dropout_b.backward(dout)\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: dropout_b.forward(xx, True, seed=1), x, dout)\n",
    "\n",
    "# The error should not be larger than 1e-10\n",
    "print ('dx relative error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing cascaded layers: FC + GeLU [2pt]\n",
    "Please find the `TestFCGeLU` function in `lib/mlp/fully_conn.py`. <br />\n",
    "You only need to complete a few lines of code in the TODO block. <br />\n",
    "Please design an `Flatten -> FC -> GeLU` network where the parameters of them match the given x, w, and b. <br />\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively. Here you only modify the param_name part, the `_w`, and `_b` are automatically assigned during network setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "x = np.random.randn(3, 5, 3)  # the input features\n",
    "w = np.random.randn(15, 5)   # the weight of fc layer\n",
    "b = np.random.randn(5)       # the bias of fc layer\n",
    "dout = np.random.randn(3, 5) # the gradients to the output, notice the shape\n",
    "\n",
    "tiny_net = TestFCGeLU()\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly #\n",
    "###################################################\n",
    "tiny_net.net.assign(\"param_name_w\", w)\n",
    "tiny_net.net.assign(\"param_name_b\", b)\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "out = tiny_net.forward(x)\n",
    "dx = tiny_net.backward(dout)\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly #\n",
    "###################################################\n",
    "dw = tiny_net.net.get_grads(\"param_name_w\")\n",
    "db = tiny_net.net.get_grads(\"param_name_b\")\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: tiny_net.forward(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: tiny_net.forward(x), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: tiny_net.forward(x), b, dout)\n",
    "\n",
    "# The errors should not be larger than 1e-7\n",
    "print (\"dx error: \", rel_error(dx_num, dx))\n",
    "print (\"dw error: \", rel_error(dw_num, dw))\n",
    "print (\"db error: \", rel_error(db_num, db))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax Function and Loss Layer [2pt]\n",
    "In the `lib/mlp/layer_utils.py`, please first complete the function `softmax`, which will be used in the function `cross_entropy`. Then, implement `corss_entropy` using `softmax`.\n",
    "Please refer to the lecture slides of the mathematical expressions of the cross entropy loss function, and complete its forward pass and backward pass. You should also take care of `size_average` on whether or not to divide by the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "num_classes, num_inputs = 6, 100\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "test_loss = cross_entropy()\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: test_loss.forward(x, y), x, verbose=False)\n",
    "\n",
    "loss = test_loss.forward(x, y)\n",
    "dx = test_loss.backward()\n",
    "\n",
    "# Test softmax_loss function. Loss should be around 1.792\n",
    "# and dx error should be at the scale of 1e-8 (or smaller)\n",
    "print (\"Cross Entropy Loss: \", loss)\n",
    "print (\"dx error: \", rel_error(dx_num, dx))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Small Fully Connected Network [2pt]\n",
    "Please find the `SmallFullyConnectedNetwork` function in `lib/mlp/fully_conn.py`. <br />\n",
    "Again you only need to complete few lines of code in the TODO block. <br />\n",
    "Please design an `FC --> GeLU --> FC` network where the shapes of parameters match the given shapes. <br />\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively. <br />\n",
    "Here you only modify the param_name part, the `_w`, and `_b` are automatically assigned during network setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = SmallFullyConnectedNetwork()\n",
    "loss_func = cross_entropy()\n",
    "\n",
    "N, D, = 4, 4  # N: batch size, D: input dimension\n",
    "H, C  = 30, 7 # H: hidden dimension, C: output dimension\n",
    "std = 0.02\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "print (\"Testing initialization ... \")\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly  #\n",
    "###################################################\n",
    "w1_std = abs(model.net.get_params(\"param_name_w\").std())\n",
    "b1 = model.net.get_params(\"param_name_b\").std()\n",
    "w2_std = abs(model.net.get_params(\"param_name_w\").std())\n",
    "b2 = model.net.get_params(\"param_name_b\").std()\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "assert w1_std < std / 10, \"First layer weights do not seem right\"\n",
    "assert np.all(b1 == 0), \"First layer biases do not seem right\"\n",
    "assert w2_std < std / 10, \"Second layer weights do not seem right\"\n",
    "assert np.all(b2 == 0), \"Second layer biases do not seem right\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing test-time forward pass ... \")\n",
    "w1 = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "w2 = np.linspace(-0.2, 0.2, num=H*C).reshape(H, C)\n",
    "b1 = np.linspace(-0.6, 0.2, num=H)\n",
    "b2 = np.linspace(-0.9, 0.1, num=C)\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly  #\n",
    "###################################################\n",
    "model.net.assign(\"param_name_w\", w1)\n",
    "model.net.assign(\"param_name_b\", b1)\n",
    "model.net.assign(\"param_name_w\", w2)\n",
    "model.net.assign(\"param_name_b\", b2)\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "feats = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.forward(feats)\n",
    "correct_scores = np.asarray([[-2.33881897, -1.92174121, -1.50466344, -1.08758567, -0.6705079, -0.25343013,  0.16364763],\n",
    "                             [-1.57214916, -1.1857013 , -0.79925345, -0.41280559, -0.02635774, 0.36009011,  0.74653797],\n",
    "                             [-0.80178618, -0.44604469, -0.0903032 ,  0.26543829,  0.62117977, 0.97692126,  1.33266275],\n",
    "                             [-0.00331319,  0.32124836,  0.64580991,  0.97037146,  1.29493301, 1.61949456,  1.94405611]])\n",
    "scores_diff = np.sum(np.abs(scores - correct_scores))\n",
    "assert scores_diff < 1e-6, \"Your implementation might be wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the loss ...\",)\n",
    "y = np.asarray([0, 5, 1, 4])\n",
    "loss = loss_func.forward(scores, y)\n",
    "dLoss = loss_func.backward()\n",
    "correct_loss = 2.4248995879903195\n",
    "assert abs(loss - correct_loss) < 1e-10, \"Your implementation might be wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the gradients (error should be no larger than 1e-6) ...\")\n",
    "din = model.backward(dLoss)\n",
    "for layer in model.net.layers:\n",
    "    if not layer.params:\n",
    "        continue\n",
    "    for name in sorted(layer.grads):\n",
    "        f = lambda _: loss_func.forward(model.forward(feats), y)\n",
    "        grad_num = eval_numerical_gradient(f, layer.params[name], verbose=False)\n",
    "        print ('%s relative error: %.2e' % (name, rel_error(grad_num, layer.grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Fully Connected Network regularized with Dropout [2pt]\n",
    "Please find the `DropoutNet` function in `fully_conn.py` under `lib/mlp` directory. <br />\n",
    "For this part you don't need to design a new network, just simply run the following test code. <br />\n",
    "If something goes wrong, you might want to double check your dropout implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "N, D, C = 3, 15, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for keep_prob in [0, 0.25, 0.5]:\n",
    "    np.random.seed(seed=seed)\n",
    "    print (\"Dropout p =\", keep_prob)\n",
    "    model = DropoutNet(keep_prob=keep_prob, seed=seed)\n",
    "    loss_func = cross_entropy()\n",
    "    output = model.forward(X, True, seed=seed)\n",
    "    loss = loss_func.forward(output, y)\n",
    "    dLoss = loss_func.backward()\n",
    "    dX = model.backward(dLoss)\n",
    "    grads = model.net.grads\n",
    "\n",
    "    print (\"Error of gradients should be around or less than 1e-3\")\n",
    "    for name in sorted(grads):\n",
    "        if name not in model.net.params.keys():\n",
    "            continue\n",
    "        f = lambda _: loss_func.forward(model.forward(X, True, seed=seed), y)\n",
    "        grad_num = eval_numerical_gradient(f, model.net.params[name], verbose=False, h=1e-5)\n",
    "        print (\"{} relative error: {}\".format(name, rel_error(grad_num, grads[name])))\n",
    "    print ()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Network\n",
    "In this section, we defined a `TinyNet` class for you to fill in the TODO block in `lib/mlp/fully_conn.py`.\n",
    "* Here please design a two layer fully connected network with Leaky ReLU activation (`Flatten --> FC --> GeLU --> FC`).\n",
    "* You can adjust the number of hidden neurons, batch_size, epochs, and learning rate decay parameters.\n",
    "* Please read the `lib/train.py` carefully and complete the TODO blocks in the `train_net` function first. Codes in \"Test a Small Fully Connected Network\" can be helpful.\n",
    "* Implement SGD in `lib/optim.py`, you will be asked to complete weight decay and Adam in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange the data\n",
    "data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"], data[\"labels_train\"]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shape:\", data[\"data_train\"].shape)\n",
    "print(\"Flattened data input size:\", np.prod(data[\"data_train\"].shape[1:]))\n",
    "print(\"Number of data classes:\", max(data['labels_train']) + 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train the network to achieve at least 30% validation accuracy [5pt]\n",
    "You may only adjust the hyperparameters inside the TODO block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = TinyNet()\n",
    "loss_f = cross_entropy()\n",
    "optimizer = SGD(model.net, 0.1)\n",
    "\n",
    "results = None\n",
    "#############################################################################\n",
    "# TODO: Use the train_net function you completed to train a network         #\n",
    "#############################################################################\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "lr_decay = 0.99\n",
    "lr_decay_every = 100\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "results = train_net(data_dict, model, loss_f, optimizer, batch_size, epochs,\n",
    "                    lr_decay, lr_decay_every, show_every=10000, verbose=True)\n",
    "opt_params, loss_hist, train_acc_hist, val_acc_hist = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at what names of params were stored\n",
    "print (opt_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: How to load the parameters to a newly defined network\n",
    "model = TinyNet()\n",
    "model.net.load(opt_params)\n",
    "val_acc = compute_acc(model, data[\"data_val\"], data[\"labels_val\"])\n",
    "print (\"Validation Accuracy: {}%\".format(val_acc*100))\n",
    "test_acc = compute_acc(model, data[\"data_test\"], data[\"labels_test\"])\n",
    "print (\"Testing Accuracy: {}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "loss_hist_ = loss_hist[1::100] # sparse the curve a bit\n",
    "plt.plot(loss_hist_, '-o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(train_acc_hist, '-o', label='Training')\n",
    "plt.plot(val_acc_hist, '-o', label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Optimizers and Regularization Techniques\n",
    "There are several more advanced optimizers than vanilla SGD, and there are many regularization tricks. You'll implement them in this section.\n",
    "Please complete the TODOs in the `lib/optim.py`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Weight Decay [2pt]\n",
    "The update rule of SGD plus weigh decay is as shown below:  \n",
    "\\begin{align*}\n",
    "\\theta_{t+1} &= \\theta_t - \\eta \\nabla_{\\theta}J(\\theta_t) - \\lambda \\theta_t\n",
    "\\end{align*}\n",
    "Update the `SGD()` function in `lib/optim.py`, and also incorporate weight decay options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the implementation of SGD with Momentum\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "N, D = 4, 5\n",
    "test_sgd = sequential(fc(N, D, name=\"sgd_fc\"))\n",
    "\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "\n",
    "test_sgd.layers[0].params = {\"sgd_fc_w\": w}\n",
    "test_sgd.layers[0].grads = {\"sgd_fc_w\": dw}\n",
    "\n",
    "\n",
    "test_sgd_wd = SGD(test_sgd, 1e-3, 1e-4)\n",
    "test_sgd_wd.step()\n",
    "\n",
    "updated_w = test_sgd.layers[0].params[\"sgd_fc_w\"]\n",
    "\n",
    "\n",
    "expected_updated_w = np.asarray([\n",
    "       [-0.39936   , -0.34678632, -0.29421263, -0.24163895, -0.18906526],\n",
    "       [-0.13649158, -0.08391789, -0.03134421,  0.02122947,  0.07380316],\n",
    "       [ 0.12637684,  0.17895053,  0.23152421,  0.28409789,  0.33667158],\n",
    "       [ 0.38924526,  0.44181895,  0.49439263,  0.54696632,  0.59954   ]])\n",
    "\n",
    "\n",
    "print ('The following errors should be around or less than 1e-6')\n",
    "print ('updated_w error: ', rel_error(updated_w, expected_updated_w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing SGD and SGD with Weight Decay [2pt]\n",
    "Run the following code block to train a multi-layer fully connected network with both SGD and SGD plus Weight Decay.\n",
    "You are expected to see Weight Decay have better validation accuracy than vinilla SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "# Arrange a small data\n",
    "num_train = 20000\n",
    "small_data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"][:num_train], data[\"labels_train\"][:num_train]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}\n",
    "\n",
    "reset_seed(seed=seed)\n",
    "model_sgd      = FullyConnectedNetwork()\n",
    "loss_f_sgd     = cross_entropy()\n",
    "optimizer_sgd  = SGD(model_sgd.net, 0.01)\n",
    "print (\"Training with Vanilla SGD...\")\n",
    "results_sgd = train_net(small_data_dict, model_sgd, loss_f_sgd, optimizer_sgd, batch_size=100, \n",
    "                        max_epochs=50, show_every=10000, verbose=True)\n",
    "\n",
    "reset_seed(seed=seed)\n",
    "model_sgdw     = FullyConnectedNetwork()\n",
    "loss_f_sgdw    = cross_entropy()\n",
    "optimizer_sgdw = SGD(model_sgdw.net, 0.01, 1e-4)\n",
    "print (\"\\nTraining with SGD plus Weight Decay...\")\n",
    "results_sgdw = train_net(small_data_dict, model_sgdw, loss_f_sgdw, optimizer_sgdw, batch_size=100, \n",
    "                         max_epochs=50, show_every=10000, verbose=True)\n",
    "\n",
    "opt_params_sgd,  loss_hist_sgd,  train_acc_hist_sgd,  val_acc_hist_sgd  = results_sgd\n",
    "opt_params_sgdw, loss_hist_sgdw, train_acc_hist_sgdw, val_acc_hist_sgdw = results_sgdw\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd, 'o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgdw, 'o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with L1 Regularization [2pts]\n",
    "With L1 Regularization, your regularized loss becomes $\\tilde{J}_\\mathrm{\\ell_1}(\\theta)$ and it's defined as\n",
    "$$\n",
    "\\tilde{J}_\\mathrm{\\ell_1}(\\theta) = J(\\theta) + \\lambda \\|\\theta\\|_{\\ell_1}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\|\\theta\\|_{\\ell_1} = \\sum_{l=1}^n \\sum_{k=1}^{n_l} |\\theta_{l,k}|\n",
    "$$\n",
    "Please implmemt TODO block of ``apply_l1_regularization`` in ``lib/layer_utils``. Such regularization funcationality is called after gradient gathering in the ``backward`` process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed(seed=seed)\n",
    "model_sgd_l1     = FullyConnectedNetwork()\n",
    "loss_f_sgd_l1    = cross_entropy()\n",
    "optimizer_sgd_l1 = SGD(model_sgd_l1.net, 0.01)\n",
    "\n",
    "print (\"\\nTraining with SGD plus L1 Regularization...\")\n",
    "results_sgd_l1 = train_net(small_data_dict, model_sgd_l1, loss_f_sgd_l1, optimizer_sgd_l1, batch_size=100, \n",
    "                         max_epochs=50, show_every=10000, verbose=True, regularization=\"l1\", reg_lambda=1e-3)\n",
    "\n",
    "opt_params_sgd_l1, loss_hist_sgd_l1, train_acc_hist_sgd_l1, val_acc_hist_sgd_l1= results_sgd_l1\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd, 'o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd_l1, 'o', label=\"SGD with L1 Regularization\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd_l1, '-o', label=\"SGD with L1 Regularization\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd_l1, '-o', label=\"SGD with L1 Regularization\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with L2 Regularization [2pts]\n",
    "With L2 Regularization, your regularized loss becomes $\\tilde{J}_\\mathrm{\\ell_2}(\\theta)$ and it's defined as\n",
    "$$\n",
    "\\tilde{J}_\\mathrm{\\ell_2}(\\theta) = J(\\theta) + \\lambda \\|\\theta\\|_{\\ell_2}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\|\\theta\\|_{\\ell_2} = \\sum_{l=1}^n \\sum_{k=1}^{n_l} \\theta_{l,k}^2\n",
    "$$\n",
    "Similarly, implmemt TODO block of ``apply_l2_regularization`` in ``lib/layer_utils``.\n",
    "For SGD, you're also asked to find the $\\lambda$ for L2 Regularization such that it achives the EXACTLY SAME effect as weight decay in the previous cells. As a reminder, learning rate is the same as previously, and the weight decay paramter was 1e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed(seed=seed)\n",
    "model_sgd_l2     = FullyConnectedNetwork()\n",
    "loss_f_sgd_l2    = cross_entropy()\n",
    "optimizer_sgd_l2 = SGD(model_sgd_l2.net, 0.01)\n",
    "#####################################################################\n",
    "#### Find lambda for L2 regularization so that                   ####\n",
    "#### it achieves EXACTLY THE SAME learning curve as weight decay ####\n",
    "l2_lambda = None\n",
    "#####################################################################\n",
    "\n",
    "print (\"\\nTraining with SGD plus L2 Regularization...\")\n",
    "results_sgd_l2 = train_net(small_data_dict, model_sgd_l2, loss_f_sgd_l2, optimizer_sgd_l2, batch_size=100, \n",
    "                           max_epochs=50, show_every=10000, verbose=False, regularization=\"l2\", reg_lambda=l2_lambda)\n",
    "\n",
    "opt_params_sgd_l2, loss_hist_sgd_l2, train_acc_hist_sgd_l2, val_acc_hist_sgd_l2 = results_sgd_l2\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgdw, 'o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd_l1, 'o', label=\"SGD with L1 Regularization\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd_l1, '-o', label=\"SGD with L1 Regularization\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd_l1, '-o', label=\"SGD with L1 Regularization\")\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd_l2, 'o', label=\"SGD with L2 Regularization\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd_l2, '-o', label=\"SGD with L2 Regularization\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd_l2, '-o', label=\"SGD with L2 Regularization\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam [2pt]\n",
    "The update rule of Adam is as shown below:  \n",
    "\\begin{align*}\n",
    "t &= t + 1 \\\\\n",
    "g_t &: \\text{gradients at update step } t \\\\\n",
    "m_t &= \\beta_1m_{t-1} + (1-\\beta_1)g_t \\\\\n",
    "v_t &= \\beta_2v_{t-1} + (1-\\beta_2)g_t^2 \\\\\n",
    "\\hat{m_t} &= m_t / (1 - \\beta_1^t) \\\\\n",
    "\\hat{v_t} &= v_t / (1 - \\beta_2^t) \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\eta\\ \\hat{m_t}}{\\sqrt{\\hat{v_t}}+\\epsilon} \\\\\n",
    "\\end{align*}\n",
    "Complete the `Adam()` function in `lib/optim.py`\n",
    "Important Notes:\n",
    "1) $t$ must be updated before everything else\n",
    "2) $\\beta_1^t$ is $\\beta_1$ exponentiated to the $t$'th power\n",
    "3) You should also enable weight decay in Adam, similar to what you did in SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Test Adam implementation; you should see errors around 1e-7 or less\n",
    "N, D = 4, 5\n",
    "test_adam = sequential(fc(N, D, name=\"adam_fc\"))\n",
    "\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "test_adam.layers[0].params = {\"adam_fc_w\": w}\n",
    "test_adam.layers[0].grads = {\"adam_fc_w\": dw}\n",
    "\n",
    "opt_adam = Adam(test_adam, 1e-2, 0.9, 0.999, t=5)\n",
    "opt_adam.mt = {\"adam_fc_w\": m}\n",
    "opt_adam.vt = {\"adam_fc_w\": v}\n",
    "opt_adam.step()\n",
    "\n",
    "updated_w = test_adam.layers[0].params[\"adam_fc_w\"]\n",
    "mt = opt_adam.mt[\"adam_fc_w\"]\n",
    "vt = opt_adam.vt[\"adam_fc_w\"]\n",
    "\n",
    "expected_updated_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print ('The following errors should be around or less than 1e-7')\n",
    "print ('updated_w error: ', rel_error(expected_updated_w, updated_w))\n",
    "print ('mt error: ', rel_error(expected_m, mt))\n",
    "print ('vt error: ', rel_error(expected_v, vt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Weight Decay v.s. L2 Regularization in Adam [5pt]\n",
    "Run the following code block to compare the plotted results between effects of weight decay and L2 regularization on Adam. Are they still the same? (we can make them the same as in SGD, can we also do it in Adam?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "reset_seed(seed)\n",
    "model_adam_wd      = FullyConnectedNetwork()\n",
    "loss_f_adam_wd     = cross_entropy()\n",
    "optimizer_adam_wd  = Adam(model_adam_wd.net, lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "\n",
    "print (\"Training with AdamW...\")\n",
    "results_adam_wd = train_net(small_data_dict, model_adam_wd, loss_f_adam_wd, optimizer_adam_wd, batch_size=100, \n",
    "                        max_epochs=50, show_every=10000, verbose=False)\n",
    "\n",
    "reset_seed(seed)\n",
    "model_adam_l2      = FullyConnectedNetwork()\n",
    "loss_f_adam_l2     = cross_entropy()\n",
    "optimizer_adam_l2 = Adam(model_adam_l2.net, lr=1e-4)\n",
    "reg_lambda_l2 = 1e-4\n",
    "print (\"\\nTraining with Adam + L2...\")\n",
    "results_adam_l2 = train_net(small_data_dict, model_adam_l2, loss_f_adam_l2, optimizer_adam_l2, batch_size=100, \n",
    "                         max_epochs=50, show_every=10000, verbose=False, regularization='l2', reg_lambda=reg_lambda_l2)\n",
    "\n",
    "opt_params_adam_wd, loss_hist_adam_wd, train_acc_hist_adam_wd, val_acc_hist_adam_wd  = results_adam_wd\n",
    "opt_params_adam_l2, loss_hist_adam_l2, train_acc_hist_adam_l2, val_acc_hist_adam_l2 = results_adam_l2\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd, 'o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgdw, 'o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgdw, '-o', label=\"SGD with Weight Decay\")\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_adam_wd, 'o', label=\"Adam with Weight Decay\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_adam_wd, '-o', label=\"Adam with Weight Decay\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_adam_wd, '-o', label=\"Adam with Weight Decay\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_adam_l2, 'o', label=\"Adam with L2\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_adam_l2, '-o', label=\"Adam with L2\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_adam_l2, '-o', label=\"Adam with L2\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Please prepare a PDF document `problem_1_solution.pdf` in the root directory of this repository with all plots and inline answers of your solution. Concretely, the document should contain the following items in strict order:\n",
    "1. Training loss / accuracy curves for the simple neural network training with > 30% validation accuracy\n",
    "2. Plots for comparing vanilla SGD to SGD + Weight Decay, SGD + L1 and SGD + L2\n",
    "3. \"Comparing different Regularizations\" plots\n",
    "\n",
    "Note that you still need to submit the jupyter notebook with all generated solutions. We will randomly pick submissions and check that the plots in the PDF and in the notebook are equivalent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
